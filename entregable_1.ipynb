{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Union, List\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIRequestError(Exception):\n",
    "    def __init__(self, status_code, message, function_name):\n",
    "        self.status_code = status_code\n",
    "        self.message = message\n",
    "        self.function_name = function_name\n",
    "        super().__init__(f\"HTTP error {self.status_code} occurred in {self.function_name}: {self.message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advantage api configuration parameters\n",
    "base_url = os.environ.get('BASE_URL')\n",
    "token = os.environ.get('API_TOKEN')\n",
    "tickers = ['AAPL','IBM']\n",
    "interval = '60min'\n",
    "topics = 'technology, manufacturing, financial_markets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero: funcion para traer serie intradiaria en 60 min del stock que necesito\n",
    "\n",
    "def intraday_stock_serie(symbol:str, interval:str):   \n",
    "    endpoint = 'TIME_SERIES_INTRADAY'\n",
    "    adjusted=True\n",
    "    extended_hours=False\n",
    "    size = 'compact'\n",
    "    parameters_market_data = {'function':endpoint, 'symbol':symbol, 'interval':interval, 'extended_hours':extended_hours,\n",
    "            'adjusted':adjusted,'outputsize':size,'apikey':token }\n",
    "    try:\n",
    "        print(\"llamando a la API ...\")\n",
    "        r = requests.get(base_url, params=parameters_market_data)\n",
    "        r.raise_for_status() \n",
    "        data = r.json()\n",
    "        print(\"Data recibida ...\")\n",
    "        if \"Error Message\" in data:\n",
    "            error_message = data[\"Error Message\"]\n",
    "            raise APIRequestError(r.status_code, error_message, \"intraday_stock_serie\")\n",
    "        else:\n",
    "            data = data[f'Time Series ({interval})']\n",
    "            \n",
    "            return data\n",
    "        \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        raise APIRequestError(http_err.response.status_code, http_err, \"intraday_stock_serie\")\n",
    "    except Exception as err:\n",
    "        raise APIRequestError(500, str(err), \"intraday_stock_serie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segundo: funcion para traer noticias relacionadas a ese stock\n",
    "\n",
    "def getSentiment(\n",
    "    symbol: str,\n",
    "    topics: Union[str, List[str]]\n",
    "):\n",
    "    \n",
    "    # Convierto topics en un solo string si vino en una lista de strings\n",
    "    if isinstance(topics, list):\n",
    "        topics = ','.join(topics)\n",
    "    \n",
    "    parameters_news_sentiment_data = {\n",
    "        'function': 'NEWS_SENTIMENT',\n",
    "        'tickers': symbol,\n",
    "        'topics': topics,\n",
    "        'apikey': token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(base_url, params=parameters_news_sentiment_data)\n",
    "        r.raise_for_status() \n",
    "        data = r.json()\n",
    "        data_feed = data['feed']\n",
    "        data_sentiment = []\n",
    "        for i in data_feed:\n",
    "            for item in i['ticker_sentiment']:\n",
    "                if item['ticker'] == symbol:\n",
    "                    # Formateo time_published \n",
    "                    time_published = datetime.strptime(i['time_published'], '%Y%m%dT%H%M%S')\n",
    "                    formatted_time_published = time_published.strftime('%Y-%m-%d %H:%M')\n",
    "                    data_sentiment.append({\n",
    "                        'ticker': item['ticker'],\n",
    "                        'time_published': formatted_time_published,\n",
    "                        'source_domain': i['source_domain'],\n",
    "                        'relevance_score': item['relevance_score'],\n",
    "                        'ticker_sentiment_label': item['ticker_sentiment_label']\n",
    "                    })\n",
    "        return data_sentiment\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        raise APIRequestError(http_err.response.status_code, http_err, \"getSentiment\")\n",
    "    except Exception as err:\n",
    "        raise APIRequestError(500, str(err), \"getSentiment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Unifico las funciones de market data y news data en una sola \n",
    "def get_stock_data(tickers, interval, topics):\n",
    "    stock_data_frames = {}\n",
    "    # Get database connection parameters\n",
    "    DB_NAME = os.environ.get('DB_NAME')\n",
    "    DB_USER = os.environ.get('DB_USER')\n",
    "    DB_PWD = os.environ.get('DB_PWD')\n",
    "    DB_PORT = os.environ.get('DB_PORT')\n",
    "    DB_HOST = os.environ.get('DB_HOST')\n",
    "    dbschema = f'{DB_USER}'\n",
    "\n",
    "    # Create the connection engine outside the loop\n",
    "    print(\"Creando la conexion con redshift\")\n",
    "    conn = sa.create_engine(\n",
    "        f\"postgresql://{DB_USER}:{DB_PWD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\",\n",
    "        connect_args={'options': f'-csearch_path={dbschema}'}\n",
    "    )\n",
    "    print(\"Conexion con redshift realizada con Ã©xito\")\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        print(f\"Creando las tablas y conectando a la API para extraer data de {ticker} \")\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            intraday_table_name = f'stock_intraday_prices_{ticker}'\n",
    "            \n",
    "            print(f\"Creando tabla de {ticker} en redshift\")\n",
    "            #Creo tabla de precios en redshift\n",
    "            conn.execute(f\"\"\"\n",
    "                DROP TABLE IF EXISTS {intraday_table_name};\n",
    "                CREATE TABLE {intraday_table_name} (\n",
    "                    date TIMESTAMP,\n",
    "                    open_price FLOAT,\n",
    "                    high_price FLOAT,\n",
    "                    low_price FLOAT,\n",
    "                    close_price FLOAT,\n",
    "                    volume INT\n",
    "                )\n",
    "                DISTKEY(date)\n",
    "                SORTKEY(date);\n",
    "                COMMIT;\n",
    "            \"\"\")\n",
    "    \n",
    "            print(\"Tabla de price creada... \")\n",
    "            #Voy con el dataframe de news sentiment\n",
    "            \n",
    "            sentiment_table_name = f'stock_sentiment_{ticker}'\n",
    "            \n",
    "            #Creo tabla de sentiment en redshift\n",
    "            conn.execute(f\"\"\"\n",
    "            DROP TABLE IF EXISTS {sentiment_table_name};\n",
    "            CREATE TABLE {sentiment_table_name}  (\n",
    "                id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "                ticker VARCHAR,\n",
    "                time_published TIMESTAMP,\n",
    "                source_domain VARCHAR,\n",
    "                relevance_score VARCHAR,\n",
    "                ticker_sentiment_label VARCHAR\n",
    "            )\n",
    "            DISTKEY(time_published)\n",
    "            SORTKEY(time_published);\n",
    "            COMMIT;\n",
    "            ;\n",
    "            \"\"\")\n",
    "            print(\"Tabla de sentiment creada... \")\n",
    "            print(f\"Conectando con API para obtener datos de {ticker}\")\n",
    "            # Traigo intraday stock data\n",
    "            intraday_data = intraday_stock_serie(ticker, interval)\n",
    "            \n",
    "            # Convierto en pandas dataframe los precios de intraday stock\n",
    "            df_intraday = pd.DataFrame.from_dict(intraday_data, orient='index')\n",
    "            df_intraday.columns = ['open_price', 'high_price', 'low_price', 'close_price', 'volume']\n",
    "            df_intraday.reset_index(inplace=True)\n",
    "            df_intraday.rename(columns={'index': 'date'}, inplace=True)\n",
    "            df_intraday['date'] = pd.to_datetime(df_intraday['date'])\n",
    "            \n",
    "            # Traigo sentiment data\n",
    "            sentiment_data = getSentiment(ticker, topics)\n",
    "            df_sentiment = pd.DataFrame(sentiment_data)\n",
    "            \n",
    "            # Guardo los df en diccionarios\n",
    "            stock_data_frames[ticker] = {\n",
    "                'intraday_data': df_intraday,\n",
    "                'sentiment_data': df_sentiment\n",
    "            }\n",
    "            print(f\"Datos de {ticker} guardados en diccionario\")\n",
    "        except APIRequestError as api_err:\n",
    "            print(f\"{api_err.function_name}: API Request Error - Status Code {api_err.status_code}: {api_err.message}\")\n",
    "            # You can handle the error based on the status code here.\n",
    "            # For example, you may choose to skip the stock if the error is not recoverable.\n",
    "            continue\n",
    "    return stock_data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando la conexion con redshift\n",
      "Conexion con redshift realizada con Ã©xito\n",
      "Creando las tablas y conectando a la API para extraer data de AAPL \n",
      "Creando tabla de AAPL en redshift\n",
      "Tabla de price creada... \n",
      "Tabla de sentiment creada... \n",
      "Conectando con API para obtener datos de AAPL\n",
      "llamando a la API ...\n",
      "Data recibida ...\n",
      "Datos de AAPL guardados en diccionario\n",
      "Creando las tablas y conectando a la API para extraer data de IBM \n",
      "Creando tabla de IBM en redshift\n",
      "Tabla de price creada... \n",
      "Tabla de sentiment creada... \n",
      "Conectando con API para obtener datos de IBM\n",
      "llamando a la API ...\n",
      "Data recibida ...\n",
      "Datos de IBM guardados en diccionario\n"
     ]
    }
   ],
   "source": [
    "data_frames_by_ticker = get_stock_data(tickers, interval, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_table(ticker, interval, topics):\n",
    "\n",
    "        #creo conn\n",
    "        print(\"Conectando con Redshift ... \")\n",
    "        DB_NAME = os.environ.get('DB_NAME')\n",
    "        DB_USER = os.environ.get('DB_USER')\n",
    "        DB_PWD = os.environ.get('DB_PWD')\n",
    "        DB_PORT = os.environ.get('DB_PORT')\n",
    "        DB_HOST = os.environ.get('DB_HOST')\n",
    "        dbschema = f'{DB_USER}'\n",
    "\n",
    "        conn = create_engine(f\"postgresql://{DB_USER}:{DB_PWD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\",\n",
    "                connect_args={'options': f'-csearch_path={dbschema}'})\n",
    "        print(\"Conexion establecida OK\")\n",
    "        \n",
    "        #llamo a la API para traer data y llenar la tabla\n",
    "        new_data_intraday = intraday_stock_serie(ticker, interval)\n",
    "        new_data_sentiment = getSentiment(ticker, topics)\n",
    "        \n",
    "        #creo df\n",
    "        new_data_df_intraday = pd.DataFrame.from_dict(new_data_intraday, orient='index')\n",
    "        new_data_df_intraday.columns = ['open_price', 'high_price', 'low_price', 'close_price', 'volume']\n",
    "        new_data_df_intraday.reset_index(inplace=True)\n",
    "        new_data_df_intraday.rename(columns={'index': 'date'}, inplace=True)\n",
    "        new_data_df_intraday['date'] = pd.to_datetime(new_data_df_intraday['date'])\n",
    "        \n",
    "        new_data_df_sentiment = pd.DataFrame(new_data_sentiment)\n",
    "\n",
    "\n",
    "        # Obtener la fecha mÃ¡s reciente en la tabla Redshift\n",
    "        print(\"Checkeando la fecha mas reciente en redshift\")\n",
    "        latest_date_query = f\"SELECT MAX(date) FROM stock_intraday_prices_{ticker}\"\n",
    "        latest_date_in_redshift = pd.read_sql(latest_date_query, conn)[\"max\"][0] if pd.read_sql(latest_date_query, conn)[\"max\"][0] != None else False\n",
    "        \n",
    "        if(latest_date_in_redshift):\n",
    "            print(\"fecha query redshift\",latest_date_in_redshift)\n",
    "            print(\"latest data redshift: \",latest_date_in_redshift, \"latest data API: \",new_data_df_intraday.date.max())\n",
    "            #Filtrar los nuevos datos para incluir solo registros con fechas posteriores a la fecha mÃ¡s reciente en Redshift\n",
    "            print(\"Filtrando la data para subir solo los updates mas recientes a redshift\")\n",
    "            print(new_data_df_intraday.date.max(), latest_date_in_redshift)\n",
    "            new_data_switch = new_data_df_intraday.date.max() > latest_date_in_redshift\n",
    "            mask_date = new_data_df_intraday['date']> latest_date_in_redshift\n",
    "            new_data_df_intraday = new_data_df_intraday[mask_date]\n",
    "            \n",
    "            if (new_data_switch):\n",
    "                #nombre de las tablas\n",
    "                intraday_table_name = f'stock_intraday_prices_{ticker}'\n",
    "                sentiment_table_name = f'stock_sentiment_{ticker}'\n",
    "\n",
    "                # Mando df a la tabla con to_sql de pandas\n",
    "                print(\"nueva data:\",new_data_df_intraday)\n",
    "\n",
    "                print(\"cargando data a la tabla con to_sql de pandas\")\n",
    "                new_data_df_intraday.to_sql(f\"{intraday_table_name}\".lower(), conn,index=False,method='multi', if_exists='append')\n",
    "                new_data_df_sentiment.to_sql(f\"{sentiment_table_name}\".lower(), conn,index=False,method='multi', if_exists='append')\n",
    "                print(\"data cargada OK\")\n",
    "            else:\n",
    "                print(\"no hay nueva data\")\n",
    "        else:\n",
    "            #nombre de las tablas\n",
    "            intraday_table_name = f'stock_intraday_prices_{ticker}'\n",
    "            sentiment_table_name = f'stock_sentiment_{ticker}'\n",
    "            print(\"cargando data a la tabla con to_sql de pandas\")\n",
    "            new_data_df_intraday.to_sql(f\"{intraday_table_name}\".lower(), conn,index=False,method='multi', if_exists='append')\n",
    "            new_data_df_sentiment.to_sql(f\"{sentiment_table_name}\".lower(), conn,index=False,method='multi', if_exists='append')\n",
    "            print(f\"Se cargaron {len(new_data_df_intraday)} filas en la tabla stock_prices_{ticker} y {len(new_data_df_sentiment)} en la tabla stock_sentiment_{ticker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advantage api configuration parameters\n",
    "base_url = os.environ.get('BASE_URL')\n",
    "token = os.environ.get('API_TOKEN')\n",
    "tickers = ['AAPL','IBM']\n",
    "interval = '60min'\n",
    "topics = 'technology, manufacturing, financial_markets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando con Redshift ... \n",
      "Conexion establecida OK\n",
      "llamando a la API ...\n",
      "Data recibida ...\n",
      "Checkeando la fecha mas reciente en redshift\n",
      "cargando data a la tabla con to_sql de pandas\n",
      "Se cargaron 100 filas en la tabla stock_prices_AAPL y 50 en la tabla stock_sentiment_AAPL\n"
     ]
    }
   ],
   "source": [
    "interval = '60min'\n",
    "topics = 'technology, manufacturing, financial_markets'\n",
    "fill_table(\"AAPL\",interval, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7546de2d5b66e6bb4051f93f462cf1f21acc5c70ac66712f7c9fa60f386dd80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
